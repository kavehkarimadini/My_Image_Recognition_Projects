{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images belonging to 2 classes.\n",
      "Found 758 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaveh\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\kaveh\\AppData\\Local\\Temp\\ipykernel_9876\\1381818809.py:65: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23/23 [==============================] - 260s 11s/step - loss: 0.5989 - accuracy: 0.8926 - val_loss: 0.4889 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "23/23 [==============================] - 240s 10s/step - loss: 0.4545 - accuracy: 0.9917 - val_loss: 0.3248 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "23/23 [==============================] - 238s 10s/step - loss: 0.3408 - accuracy: 0.9848 - val_loss: 0.2513 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e8e73e380>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "# define the number of output classes\n",
    "num_classes = 1\n",
    "\n",
    "# create an instance of the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a new fully connected layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "# create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# freeze the weights of the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# create an instance of the ImageDataGenerator class\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "val_generator= datagen.flow_from_directory(\n",
    "        'C:/newJupyterProject/kaveh_1402/MyProjects/tensorflow face recognition/people/testing',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# load the training data for both datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        'C:/newJupyterProject/kaveh_1402/MyProjects/tensorflow face recognition/people/training',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# load the training data for the second dataset\n",
    "# val_generator2= datagen.flow_from_directory(\n",
    "#         'C:/newJupyterProject/kaveh_1402/MyProjects/tensorflow face recognition/people/notkaveh0/notkavehtest',\n",
    "#         target_size=(224, 224),\n",
    "#         batch_size=32,\n",
    "#         class_mode='binary')\n",
    "# train_generator2 = datagen.flow_from_directory(\n",
    "#         'C:/newJupyterProject/kaveh_1402/MyProjects/tensorflow face recognition/people/notkaveh0/notkavehtrain',\n",
    "#         target_size=(224, 224),\n",
    "#         batch_size=32,\n",
    "#         class_mode='binary')\n",
    "\n",
    "# train the model on both datasets\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=758//32,\n",
    "        epochs=3,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 348ms/step\n",
      "[[0.72541386]]\n",
      "The image is a kaveh selfie.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# load the image\n",
    "img_path = 'MyProjects/tensorflow face recognition/kavehselfie02.jpg'\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# convert the image to a numpy array\n",
    "x = img_to_array(img)\n",
    "\n",
    "# normalize the pixel values (rescale to between 0 and 1)\n",
    "x = x / 255.0\n",
    "\n",
    "# add a batch dimension to the array\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# make the prediction\n",
    "predictions = model.predict(x)\n",
    "print(predictions)\n",
    "# print the predicted label\n",
    "if predictions[0] > 0.48:\n",
    "    print(\"The image is a kaveh selfie.\")\n",
    "else:\n",
    "    print(\"The image is not a kaveh selfie.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kaveh\\AppData\\Local\\Temp\\tmp1q0eorcb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kaveh\\AppData\\Local\\Temp\\tmp1q0eorcb\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "# load your Keras model\n",
    "#model = load_model('my_model.h5')\n",
    "\n",
    "# convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# save the TensorFlow Lite model to a file\n",
    "with open('my_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/newJupyterProject/kaveh_1402/MyProjects\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/newJupyterProject/kaveh_1402/MyProjects\\assets\n"
     ]
    }
   ],
   "source": [
    "# define the export path for TensorFlow Serving\n",
    "export_path = 'C:/newJupyterProject/kaveh_1402/MyProjects'\n",
    "\n",
    "# convert the Keras model to a TensorFlow SavedModel\n",
    "tf.saved_model.save(model, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current input shape: [1 1 1 3]\n",
      "Updated input shape: [  1 224 224   3]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"my_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input tensor index and shape\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "input_shape = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "# Print the current input shape\n",
    "print(f\"Current input shape: {input_shape}\")\n",
    "\n",
    "# Update the input shape to (1, 224, 224, 3)\n",
    "new_input_shape = (1, 224, 224, 3)\n",
    "interpreter.resize_tensor_input(input_index, new_input_shape)\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.invoke()\n",
    "\n",
    "# Print the updated input shape\n",
    "print(f\"Updated input shape: {interpreter.get_input_details()[0]['shape']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current input shape: [1 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"my_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input tensor index and shape\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "input_shape = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "# Print the current input shape\n",
    "print(f\"Current input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Interpreter' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Save the modified model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnew_model.tflite\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     f\u001b[39m.\u001b[39mwrite(interpreter\u001b[39m.\u001b[39;49mmodel()\u001b[39m.\u001b[39mserialize())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Interpreter' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "# Save the modified model\n",
    "with open('new_model.tflite', 'wb') as f:\n",
    "    f.write(interpreter.model().serialize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Interpreter' object has no attribute '_model_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m modified_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodified_model.tflite\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(modified_model_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 18\u001b[0m     f\u001b[39m.\u001b[39mwrite(interpreter\u001b[39m.\u001b[39;49m_model_content\u001b[39m.\u001b[39mserialize())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Interpreter' object has no attribute '_model_content'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"my_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details.\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "# Modify the input shape.\n",
    "interpreter.resize_tensor_input(input_details[\"index\"], [1, 224, 224, 3])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Save the modified model.\n",
    "modified_model_path = \"modified_model.tflite\"\n",
    "with open(modified_model_path, \"wb\") as f:\n",
    "    f.write(interpreter._model_content.serialize())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
